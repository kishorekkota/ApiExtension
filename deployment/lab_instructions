## Lab Instruction

## Install kubectl
curl -O https://s3.us-west-2.amazonaws.com/amazon-eks/1.24.11/2023-03-17/bin/linux/amd64/kubectl
chmod +x ./kubectl
sudo cp ./kubectl /usr/local/bin
export PATH=/usr/local/bin:$PATH


## Download the AWS CLI utility, give it executable permissions, and copy it into a directory that is part of the PATH environment variable:

curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip awscliv2.zip
sudo ./aws/install

## Install the AWS CLI version 2
aws --version

## Configure AWS CLI to fetch EKS Cluster details

EKS_CLUSTER_NAME=$(aws eks list-clusters --region us-west-2 --query clusters[0] --output text)
echo $EKS_CLUSTER_NAME

## Update Kubeconfig to point to cluster
aws eks update-kubeconfig --name $EKS_CLUSTER_NAME --region us-west-2


## Kube get nodes
kubectl get nodes


## Kube describe nodes
kubectl describe nodes

## The EKS cluster provided within this lab has been provisioned using the eksctl utility. The cluster was created with the following script and settings:

eksctl create cluster \
--version=1.24 \
--name=Cluster-1 \
--nodes=1 \
--node-type=t2.medium \
--ssh-public-key="cloudacademylab" \
--region=us-west-2 \
--zones=us-west-2a,us-west-2b,us-west-2c \
--node-volume-type=gp2 \
--node-volume-size=20


## An OpenID Connect provider needs to be established. This was performed using the following command:

eksctl utils associate-iam-oidc-provider \
--region us-west-2 \
--cluster Cluster-1 \
--approve

## A new IAM Policy was created, providing various permissions required to provision the underlying infrastructure items (ALBs and/or NLBs) created when Ingress and Service cluster resources are created.

## Note: You can view the specific set of IAM permissions granted here:
## https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/install/iam_policy.json

curl -o /tmp/iam_policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/install/iam_policy.json
aws iam create-policy \
--policy-name AWSLoadBalancerControllerIAMPolicy \
--policy-document file:///tmp/iam_policy.json

##A new cluster service account bound to the IAM policy has been created. When the AWS Load Balancer controller is later deployed by you, it will be configured to operate with this service account:

eksctl create iamserviceaccount \
--cluster Cluster-1 \
--namespace kube-system \
--name aws-load-balancer-controller \
--attach-policy-arn arn:aws:iam::${AWS::AccountId}:policy/AWSLoadBalancerControllerIAMPolicy \
--override-existing-serviceaccounts \
--approve


## Install HELM 3

{
pushd /tmp
curl -o helm-v3.9.2-linux-amd64.tar.gz https://get.helm.sh/helm-v3.9.2-linux-amd64.tar.gz
tar -xvf helm-v3.9.2-linux-amd64.tar.gz
sudo mv linux-amd64/helm /usr/local/bin/helm
popd
which helm
}

## Update the local Helm repo. In the terminal run the following command:

{
helm repo add eks https://aws.github.io/eks-charts
helm repo update
}

## Confirm that a cluster service account exists for the AWS Load Balancer Controller - created during lab startup time. In the terminal run the following command:

kubectl get sa aws-load-balancer-controller -n kube-system -o yaml

## Retrieve the VPC ID for the VPC that the EKS cluster worker node is deployed within

{
VPC_ID=$(aws eks describe-cluster \
--name Cluster-1 \
--query "cluster.resourcesVpcConfig.vpcId" \
--output text)
echo $VPC_ID
}

## Deploy the Custom Resource Definition (CRD) resources required by the AWS Load Balancer Controller. In the terminal run the following command:

kubectl apply -k "github.com/aws/eks-charts/stable/aws-load-balancer-controller/crds?ref=master"


## Using the helm command, deploy the AWS Load Balancer Controller chart. In the terminal run the following command:

helm install \
aws-load-balancer-controller \
eks/aws-load-balancer-controller \
-n kube-system \
--set clusterName=Cluster-1 \
--set serviceAccount.create=false \
--set serviceAccount.name=aws-load-balancer-controller \
--set image.tag=v2.4.2 \
--set region=us-west-2 \
--set vpcId=${VPC_ID} \
--version=1.4.3


## Confirm that the AWS Load Balancer Controller is deployed and running. In the terminal run the following command:

kubectl -n kube-system rollout status deployment aws-load-balancer-controller


## Examine details of the deployed AWS Load Balancer Controller. In the terminal run the following command:

kubectl describe deployment -n kube-system aws-load-balancer-controller

/**

Introduction
In this lab step, you'll examine and review a sample Web App that you will deploy into the cluster. In particular, you'll view the Web App Go source code, allowing you to understand how the internals of the Web App work. You'll also be shown the Dockerfile used to build the publicly available cloudacademydevops/webappecho:v3 container image.



Review
1. The sample Web App that you will deploy in the following lab step has been developed using the Go programming language. It is designed to simply to display a configurable static message on a coloured background, also configurable. Connecting to this web application is performed using HTTP (layer-7) connections. The source code that makes up the Web App is provided below to give you a basic understanding of how it works internally:

main.go

https://github.com/cloudacademy/web-echo-app/blob/main/main.go

Copy code
package main

import (
	"html/template"
	"net/http"
	"os"
)

type config struct {
	Message         string
	BackgroundColor string
}

const htmlTemplate = `
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CloudAcademy</title>
</head>
<link href="https://fonts.googleapis.com/css2?family=Montserrat&display=swap" rel="stylesheet">

<style>
body {
  position: relative;
  height: 100vh;
  margin: 0;
  background-color: {{.BackgroundColor}};
}

h1 {
 position: absolute;
 top: 40%;
 transform: translateY(-50%);
 width: 100%;
 text-align: center;
 margin: 0;
 font-family: 'Montserrat', sans-serif;
}
</style>

<body>
<h1>{{.Message}}</h1>
</body>
</html>
`

func main() {
	hostport := os.Getenv("HOSTPORT")
	message := os.Getenv("MESSAGE")
	bgcolor := os.Getenv("BACKGROUND_COLOR")

	tmpl := template.Must(template.New("main").Parse(htmlTemplate))

	http.HandleFunc("/", func(w http.ResponseWriter, r *http.Request) {
		data := config{
			Message:         message,
			BackgroundColor: bgcolor,
		}
		tmpl.Execute(w, data)
	})

	http.ListenAndServe(hostport, nil)
}
2. The sample Web App has been pre-packaged and built into a container image which you will deploy next. The following Dockerfile was used to create the cloudacademydevops/webappecho:v3 container image:

Dockerfile

https://github.com/cloudacademy/web-echo-app/blob/main/Dockerfile

Copy code
FROM golang:1.19.0-bullseye as builder

WORKDIR /go/src/webapp/
COPY main.go ./
COPY go.mod ./

RUN CGO_ENABLED=0 GOOS=linux go build -o webapp .

FROM scratch

COPY --from=builder /go/src/webapp/webapp /go/bin/

ENV HOSTPORT=0.0.0.0:8080
EXPOSE 8080

CMD ["/go/bin/webapp"]


Summary
In this lab step, you reviewed the Web App source code and the Dockerfile used to create the publicly available cloudacademydevops/webappecho:v3 container image. In the next lab step, you will deploy the Web App into the cluster and expose it externally using an Ingress resource

**/

## Create a dedicated namespace to host both versions of the sample web app. In the terminal run the following commands:

{
kubectl create ns webapp
kubectl config set-context $(kubectl config current-context) --namespace=webapp
}


## Create Config map

cat << EOF | kubectl apply -f -
apiVersion: v1
kind: ConfigMap
metadata:
  name: webapp-cfg-v1
  namespace: webapp
  labels:
    version: v1
data:
  message: "CloudAcademy.v1.0.0"
  bgcolor: "yellow"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: webapp-cfg-v2
  namespace: webapp
  labels:
    version: v2
data:
  message: "CloudAcademy.v2.0.0"
  bgcolor: "cyan"
EOF

## Check config map

kubectl get cm webapp-cfg-v1 webapp-cfg-v2

## Create Deployment

cat << EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend-v1
  namespace: webapp
  labels:
    role: frontend
    version: v1
spec:
  replicas: 2
  selector:
    matchLabels:
      role: frontend
      version: v1
  template:
    metadata:
      labels:
        role: frontend
        version: v1
    spec:
      containers:
      - name: webapp
        image: cloudacademydevops/webappecho:v3
        imagePullPolicy: IfNotPresent
        command: ["/go/bin/webapp"]
        ports:
        - containerPort: 8080
        env:
        - name: MESSAGE
          valueFrom:
            configMapKeyRef:
              name: webapp-cfg-v1
              key: message
        - name: BACKGROUND_COLOR
          valueFrom:
            configMapKeyRef:
              name: webapp-cfg-v1
              key: bgcolor
EOF

## Check deployment

{
kubectl rollout status deployment frontend-v1
kubectl rollout status deployment frontend-v2
}


## Create Service

cat << EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: frontend-v1
  namespace: webapp
  labels:
    role: frontend
    version: v1
spec:
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort: 8080
  selector:
    role: frontend
    version: v1
  type: NodePort
EOF

cat << EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: frontend-v2
  namespace: webapp
  labels:
    role: frontend
    version: v2
spec:
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort: 8080
  selector:
    role: frontend
    version: v2
  type: NodePort
EOF


## check service and endpoints

kubectl get svc,ep


## Create an Ingress resource for the V1 sample web app. In the terminal run the following command:

cat << EOF | kubectl apply -f -
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: frontend
  namespace: webapp
  annotations:
    kubernetes.io/ingress.class: alb
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
spec:
  rules:
    - http:
        paths:
          - pathType: Prefix
            path: /
            backend:
              service:
                name: frontend-v1
                port:
                  number: 80
EOF

kubectl get ingress frontend


{
ALB_FQDN=$(kubectl -n webapp get ingress frontend -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
echo ALB_FQDN=$ALB_FQDN
until nslookup $ALB_FQDN >/dev/null 2>&1; do sleep 2 && echo waiting for DNS to propagate...; done
until curl --silent $ALB_FQDN | grep CloudAcademy >/dev/null 2>&1; do sleep 2 && echo waiting for ALB to register targets...; done
curl -I $ALB_FQDN
echo
echo READY...
echo browse to:
echo " http://$ALB_FQDN"
}

## Update the Ingress resource to include annotations for routing HTTP traffic.
## Create a new file named annotations.config and populate it with path routing configuration. In the terminal run the following command

cat > annotations.config << EOF
    kubernetes.io/ingress.class: alb
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
    alb.ingress.kubernetes.io/actions.forward-tg-svc1: >
      {"type":"forward","forwardConfig":{"targetGroups":[{"serviceName":"frontend-v1","servicePort":"80"}]}}
    alb.ingress.kubernetes.io/actions.forward-tg-svc2: >
      {"type":"forward","forwardConfig":{"targetGroups":[{"serviceName":"frontend-v2","servicePort":"80"}]}}
    alb.ingress.kubernetes.io/actions.custom-path1: >
      {"type":"fixed-response","fixedResponseConfig":{"contentType":"text/plain","statusCode":"200","messageBody":"follow the white rabbit..."}}
EOF

## Capture the contents of the annotations.config file in a variable named ANNOTATIONS. In the terminal run the following command:

{
ANNOTATIONS="$(cat annotations.config)"
echo "$ANNOTATIONS"
}

## Create a new file named ingress.annotations.yaml and populate it with the Ingress manifest configuration, injecting the updated set of path routing annotations. In the terminal run the following command:

cat > ingress.annotations.yaml << EOF
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: frontend
  namespace: webapp
  annotations:
$ANNOTATIONS
spec:
  rules:
    - http:
        paths:
          - path: /yellow
            pathType: Exact
            backend:
              service:
                name: forward-tg-svc1
                port:
                  name: use-annotation
          - path: /cyan
            pathType: Exact
            backend:
              service:
                name: forward-tg-svc2
                port:
                  name: use-annotation
          - path: /white
            pathType: Exact
            backend:
              service:
                name: custom-path1
                port:
                  name: use-annotation
EOF


/**
Introduction
In this lab step, you'll examine and review a sample TCP Echo App that you deploy into the cluster. In particular, you'll view the TCP Echo App Go source code, allowing you to understand how the internals of the TCP Echo App work. You'll also be shown the Dockerfile used to build the publicly available cloudacademydevops/tcpapp:v1 container image.



Review
1. The sample TCP App that you will deploy in the following lab step has been developed using the Go programming language. It is designed to simply echo back any data that it recieves. Clients connect to this application using TCP (layer-4) connections. The source code that makes up the TCP App is provided to give you a basic understanding of how it works internally:

main.go

https://github.com/cloudacademy/tcp-echo-app/blob/main/main.go

Copy code
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
package main
import (
	"bufio"
	"fmt"
	"io"
	"net"
	"os"
)
func echo(conn net.Conn) {
	r := bufio.NewReader(conn)
	for {
		line, err := r.ReadBytes(byte('\n'))
		switch err {
		case nil:
			break
		case io.EOF:
		default:
			fmt.Println("ERROR", err)
		}
		var remoteIP = ""
		if addr, ok := conn.RemoteAddr().(*net.TCPAddr); ok {
			remoteIP = addr.IP.String()
		}
		conn.Write([]byte(remoteIP + ": " + string(line[:]) + "\n"))
	}
}
func main() {
	var hostport = os.Getenv("HOSTPORT")
	l, err := net.Listen("tcp", hostport)
	if err != nil {
		fmt.Println("ERROR", err)
		os.Exit(1)
	}
	for {
		conn, err := l.Accept()
		if err != nil {
			fmt.Println("ERROR", err)
			continue
		}
		go echo(conn)
	}
}

2. The sample TCP App has been pre-packaged and built into a container image which you will deploy in the next lab step. The following Dockerfile was used to create the cloudacademydevops/tcpapp:v1 container image:

Dockerfile

https://github.com/cloudacademy/tcp-echo-app/blob/main/Dockerfile

Copy code
1
2
3
4
5
6
7
8
9
10
11
FROM golang:1.19.0-bullseye as builder
WORKDIR /go/src/tcpapp/
COPY main.go ./
COPY go.mod ./
RUN CGO_ENABLED=0 GOOS=linux go build -o tcpapp .
FROM alpine:3.16.2
RUN apk add ca-certificates
COPY --from=builder /go/src/tcpapp/tcpapp /go/bin/
ENV HOSTPORT=0.0.0.0:9091
EXPOSE 9091
CMD ["/go/bin/tcpapp"]



Summary
In this lab step, you reviewed the TCP Echo App code and the Dockerfile used to create the publicly available cloudacademydevops/tcpapp:v1 container image. In the next lab step, you will deploy the TCP Echo App into the cluster and expose it externally using a Service resource.

**/


/**
Introduction
In this lab step, you'll deploy the sample TCP Echo App into the cluster. You'll then expose the TCP Echo App externally to the public using a Service resource (of type LoadBalancer) - this in turn will result in a new NLB (Network Load Balancer) being provisioned by the AWS Load Balancer Controller.

Choosing a Service (type LoadBalancer) resource to expose your cluster hosted application to the Internet, will result in the AWS Load Balancer Controller provisioning a NLB (Network Load Balancer). Your application will therefore benefit from many of the features that the NLB provides:

Client IP preservation
Able to scale to millions of requests per second
Able to respond to TCP (layer-4) traffic

Instructions
1. Create a new dedicated namespace for hosting all TCP Echo App related resources - and immediately swap into it. In the terminal run the following commands:

Copy code
{
kubectl create ns tcpapp
kubectl config set-context $(kubectl config current-context) --namespace=tcpapp
}
alt

2. Deploy the TCP Echo App into the cluster. In the terminal run the following command:
Copy code
cat << EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: tcpapp
  namespace: tcpapp
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: tcpapp
  replicas: 2
  template:
    metadata:
      labels:
        app.kubernetes.io/name: tcpapp
    spec:
      containers:
      - image: cloudacademydevops/tcpapp:v1
        imagePullPolicy: Always
        name: tcpapp
        ports:
        - containerPort: 9091
EOF
alt
3. Confirm that the deployment has been successfully performed. In the terminal run the following command:
Copy code
kubectl -n tcpapp rollout status deployment tcpapp
alt
4. Expose the TCP Echo App externally to the public. Create and deploy a new Service resource of type Load Balancer. Under the covers, the AWS Load Balancer Controller will provision a new NLB for this resource. In the terminal run the following command:

Note: Review each of the annotations used. These will be used by the AWS Load Balancer Controller to create and configure the NLB that is provisioned as a result of deploying this Service resource.
Copy code
cat << EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: tcpapp
  namespace: tcpapp
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: external
    service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: ip
    service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing
    service.beta.kubernetes.io/aws-load-balancer-target-group-attributes: preserve_client_ip.enabled=true
spec:
  ports:
  - port: 80
    targetPort: 9091
    protocol: TCP
  type: LoadBalancer
  selector:
    app.kubernetes.io/name: tcpapp
EOF
alt

5. Wait for the NLB to be provisioned by the AWS Load Balancer Controller. In the terminal run the following command:
Copy code
{
NLB_FQDN=$(kubectl -n tcpapp get svc tcpapp -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
echo NLB_FQDN=$NLB_FQDN
until nslookup $NLB_FQDN >/dev/null 2>&1; do sleep 2 && echo waiting for DNS to propagate...; done
NLB_PUBLICIP=$(dig $NLB_FQDN +short | head -n1)
echo
echo NLB ready...
echo " NLB Public IP: $NLB_PUBLICIP"
}
alt
6. In the AWS EC2 Console, navigate to the Load Balancers section. Confirm the presence of a new NLB (Network Load Balancer), provisioned by the cluster's AWS Load Balancer Controller for the Service (type LoadBalancer) resource you just created:
alt


Summary
In this lab step, you deployed the sample TCP Echo App into the cluster. You'll then exposed the TCP Echo App externally to the public using a new Service resource, configured to be of type LoadBalancer. You then confirmed that the AWS Load Balancer Controller triggered the creation of a new NLB for the purpose of exposing the TCP Echo App publicly. In the next lab step you will send TCP traffic into the cluster hosted TCP Echo App via the newly provisioned NLB.

**/


## NLB Provisioning using AWS Load Balancer Controller

{
kubectl create ns tcpapp
kubectl config set-context $(kubectl config current-context) --namespace=tcpapp
}


cat << EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: tcpapp
  namespace: tcpapp
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: tcpapp
  replicas: 2
  template:
    metadata:
      labels:
        app.kubernetes.io/name: tcpapp
    spec:
      containers:
      - image: cloudacademydevops/tcpapp:v1
        imagePullPolicy: Always
        name: tcpapp
        ports:
        - containerPort: 9091
EOF


kubectl -n tcpapp rollout status deployment tcpapp


cat << EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: tcpapp
  namespace: tcpapp
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: external
    service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: ip
    service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing
    service.beta.kubernetes.io/aws-load-balancer-target-group-attributes: preserve_client_ip.enabled=true
spec:
  ports:
  - port: 80
    targetPort: 9091
    protocol: TCP
  type: LoadBalancer
  selector:
    app.kubernetes.io/name: tcpapp
EOF


{
NLB_FQDN=$(kubectl -n tcpapp get svc tcpapp -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
echo NLB_FQDN=$NLB_FQDN
until nslookup $NLB_FQDN >/dev/null 2>&1; do sleep 2 && echo waiting for DNS to propagate...; done
NLB_PUBLICIP=$(dig $NLB_FQDN +short | head -n1)
echo
echo NLB ready...
echo " NLB Public IP: $NLB_PUBLICIP"
}



ntroduction
In this lab step, you'll connect to the cluster hosted TCP Echo App via the newly provisioned NLB. You'll be able to test TCP based connectivity to the TCP Echo App - confirming that it can be accessed externally. The client server that you connect from has been configured with 2 seperate NICs (Network Interfaces) - each configured with a different public IP address. You'll be instructed to update the local routing table such that traffic sent to the cluster hosted TCP Echo App (via the NLB) is recieved from the different IP addresses. This in turn will allow you to observe how the client IP address of the sender is maintained and passed downstream from the NLB to the TCP Echo App pods, which will report and echo this back to the sender.



Instructions
1. Confirm that the host you are currently working on has eth0 and eth1 interfaces attached. In the terminal run the following command:

Copy code
ip -o link show | grep "eth0\|eth1"
alt

2. Retrieve the eth0 and eth1 interface MAC addresses. In the terminal run the following commands:
Copy code
{
ETH0_MAC=`ip -o link show | grep eth0 | cut -d' ' -f20`
ETH1_MAC=`ip -o link show | grep eth1 | cut -d' ' -f20`
echo eth0: $ETH0_MAC
echo eth1: $ETH1_MAC
echo
}
alt

3. Retrieve the eth0 and eth1 assigned public IP addresses. In the terminal run the following commands:
Copy code
{
ETH0_PUBLICIP=`curl -s 169.254.169.254/latest/meta-data/network/interfaces/macs/$ETH0_MAC/public-ipv4s`
ETH1_PUBLICIP=`curl -s 169.254.169.254/latest/meta-data/network/interfaces/macs/$ETH1_MAC/public-ipv4s`
echo eth0: $ETH0_PUBLICIP
echo eth1: $ETH1_PUBLICIP
echo
}
alt
4. Display the current routing table for the local host. In the terminal run the following command:
Copy code
sudo ip route show
alt
5. Add a new route into the local host routing table to specifically route NLB traffic via the eth0 interface. In the terminal run the following commands:
Copy code
{
#eth0
sudo ip route add $NLB_PUBLICIP via 192.168.64.1 dev eth0
sudo ip route show
echo
}
alt
6. Install the netcat utility, this will allow you to send and recieve TCP based traffic. In the terminal run the following command:
Copy code
sudo yum install -y nc
alt
7. Send data to the TCP Echo App via the eth0 interface.

7.1. Connect to the cluster hosted TCP Echo App (via the NLB). In the terminal run the following command:
Copy code
nc $NLB_PUBLICIP 80
alt
7.2. Type hello and hit the Return/Enter key.

7.3. Confirm that the TCP Echo App echoes back the data hello, together with the client's eth0 assigned public IP address.
alt
Note:  If you receive a Connection timed out error the first IP address returned by dig for the NLB may have been removedalt
To successfully establish a connection in this case issue the following commands (and enter hello at the prompt) to attempt again using the most recent returned IP address for the NLB
Copy code
1
2
3
4
sudo ip route delete $NLB_PUBLICIP
NLB_PUBLICIP=$(dig $NLB_FQDN +short)
sudo ip route add $NLB_PUBLICIP via 192.168.64.1 dev eth0
nc $NLB_PUBLICIP 80

7.4. Optional - send additional data to the TCP Echo App and continue to observe the echoed responses.

7.5. Enter the CTRL+C key sequence to end the current netcat process.
alt
8. Update the local host routing table to specifically route NLB traffic via the eth1 interface. In the terminal run the following commands:
Copy code
{
#eth1
sudo ip route delete $NLB_PUBLICIP
sudo ip route add $NLB_PUBLICIP via 192.168.64.1 dev eth1
sudo ip route show
echo
}
alt
9. Send data to the TCP Echo App via the eth1 interface.

9.1. Connect to the cluster hosted TCP Echo App (via the NLB). In the terminal run the following command:
Copy code
nc $NLB_PUBLICIP 80
alt
9.2. Type hello and hit the Return/Enter key:

9.3. Confirm that the TCP Echo App echoes back the data hello, together with the client's eth1 assigned public IP address.
alt
9.4. Optional - send additional data to the TCP Echo App and continue to observe the echoed responses.

9.5. Enter the CTRL+C key sequence to end the current netcat process.
alt

Summary
In this lab step, you used the netcat utility to test and confirm TCP based connectivity against the cluster hosted TCP Echo App. TCP based traffic was sent and received via the NLB - created as a consequence of deploying a Service cluster resource to expose the TCP Echo App. You altered the local host routing table to send traffic from either the eth0 or eth1 network interfaces, each having been assigned their own unique public IP addresses. This allowed you to send TCP traffic egressing from different public IP sender addresses, allowing you to confirm that the client IP address is indeed preserved as it traverses through the NLB and into the cluster pods. The cluster hosted TCP Echo App was designed to simply echo back any data it receives, together with the client's public IP address.